model_name: distilbert-base-uncased
train_file: "./data/train.txt"
val_file: "./data/val.txt"
output_dir: "./output"
max_seq_length: 128
batch_size: 16
learning_rate: 3e-5
num_epochs: 3
gradient_accumulation_steps: 2
log_interval: 100