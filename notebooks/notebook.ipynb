{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Interpretability and Evaluation Demo\n",
        "This notebook demonstrates how to:\n",
        "- Load a trained model checkpoint.\n",
        "- Evaluate sample predictions.\n",
        "- Visualize attention weights.\n",
        "- Explore basic metrics like perplexity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {}
      },
      "outputs": [],
      "source": [
        "# If running in colab or a fresh environment, you might need to install packages\n",
        "# !pip install transformers accelerate torch datasets tqdm plotly\n",
        "import torch\n",
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import plotly.io as pio\n",
        "pio.renderers.default = \"notebook\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Model and Tokenizer\n",
        "Assuming a checkpoint was saved under `output/epoch_2/` or similar.\n",
        "Replace the path if necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {}
      },
      "outputs": [],
      "source": [
        "model_path = \"../output/epoch_2\"  # Adjust this if needed\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_path)\n",
        "model.eval()\n",
        "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sample Predictions\n",
        "Let's test the model on a few masked sentences to see if it predicts reasonable tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {}
      },
      "outputs": [],
      "source": [
        "examples = [\n",
        "    \"The capital of France is [MASK].\",\n",
        "    \"Machine learning is a field of [MASK] intelligence.\",\n",
        "    \"The [MASK] brown fox jumps over the lazy dog.\"\n",
        "]\n",
        "\n",
        "for text in examples:\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
        "    predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
        "    predicted_token = tokenizer.decode(predicted_token_id)\n",
        "    print(f\"Input: {text}\\nPrediction: {predicted_token}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute Perplexity on a Small Sample\n",
        "For masked language modeling, perplexity is not always directly computed. If you use a causal LM, you can measure perplexity by evaluating the log-likelihood of a dataset.\n",
        "\n",
        "Here, we’ll do a rough approximation: we’ll mask random tokens and see if the model predicts them correctly, treating that as an indication of model confidence.\n",
        "For a true perplexity calculation with a masked LM, you'd implement the standard MLM loss calculation on a validation set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {}
      },
      "outputs": [],
      "source": [
        "# Example approximate perplexity computation:\n",
        "# We'll pretend each prediction probability for the masked token can give us a sense.\n",
        "\n",
        "test_text = \"Deep learning models rely heavily on large amounts of training data.\"  # Adjust as needed\n",
        "inputs = tokenizer(test_text, return_tensors=\"pt\").to(model.device)\n",
        "input_ids = inputs.input_ids.clone()\n",
        "\n",
        "# Randomly mask a few tokens (except special tokens)\n",
        "rnd = np.random.RandomState(42)\n",
        "maskable_positions = (input_ids[0] != tokenizer.cls_token_id) & (input_ids[0] != tokenizer.sep_token_id)\n",
        "mask_positions = rnd.choice(maskable_positions.nonzero(as_tuple=True)[0].cpu().numpy(), size=2, replace=False)\n",
        "for pos in mask_positions:\n",
        "    input_ids[0, pos] = tokenizer.mask_token_id\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids)\n",
        "logits = outputs.logits\n",
        "\n",
        "loss_values = []\n",
        "for pos in mask_positions:\n",
        "    true_id = inputs.input_ids[0, pos]\n",
        "    pred_dist = logits[0, pos]\n",
        "    # Negative log-likelihood for the correct token\n",
        "    nll = -torch.log_softmax(pred_dist, dim=-1)[true_id]\n",
        "    loss_values.append(nll.item())\n",
        "\n",
        "approx_loss = np.mean(loss_values)\n",
        "approx_ppl = np.exp(approx_loss)\n",
        "print(f\"Approximate masked perplexity on sample: {approx_ppl:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing Attention\n",
        "For interpretability, let's visualize the attention weights of the model’s last layer for one input.\n",
        "Note: Not all models output attention by default. If attention isn't returned, you might need to load the model with `output_attentions=True` or use a method like `model(**inputs, output_attentions=True)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {}
      },
      "outputs": [],
      "source": [
        "# Re-load the model with attention outputs (if supported by the model architecture)\n",
        "# If this doesn't work, check if your chosen model supports attention outputs.\n",
        "model_att = AutoModelForMaskedLM.from_pretrained(model_path, output_attentions=True).to(model.device)\n",
        "model_att.eval()\n",
        "\n",
        "test_sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "att_inputs = tokenizer(test_sentence, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    att_outputs = model_att(**att_inputs)\n",
        "attentions = att_outputs.attentions  # a tuple (layer_count, batch, heads, seq_len, seq_len)\n",
        "\n",
        "# We'll visualize attentions from the last layer\n",
        "last_layer_attn = attentions[-1][0]  # (heads, seq_len, seq_len), taking the first (and only) batch\n",
        "head_to_visualize = 0\n",
        "att_matrix = last_layer_attn[head_to_visualize].cpu().numpy()\n",
        "\n",
        "# Create a token-to-token attention heatmap\n",
        "tokens = tokenizer.convert_ids_to_tokens(att_inputs.input_ids[0])\n",
        "fig = px.imshow(att_matrix, x=tokens, y=tokens, \n",
        "                color_continuous_scale=\"RdBu\", title=\"Attention Head Visualization (Last Layer, Head 0)\")\n",
        "fig.update_xaxes(side=\"top\", tickangle=45)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretation\n",
        "We can see which tokens each token is attending to. For example, the subject token \"The\" might attend strongly to the verb or the object tokens in the sentence.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
